{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7cfab7-ed34-4330-a672-8d049a6ed8af",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Logistic Regression\n",
    "\n",
    "This notebook implements a sentiment analysis model using Logistic Regression and TF-IDF vectorization. The goal is to classify text as positive, negative, or neutral. The current model achieves an accuracy of 0.666, and improvements are planned using larger datasets and advanced models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f60ddba-e28f-4846-8831-d98e9284e06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Ati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ati\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049c267b-0a18-4e94-bb87-d73ffbcce67b",
   "metadata": {},
   "source": [
    "## Dataset and Preprocessing\n",
    "\n",
    "The dataset consists of 45 balanced text samples labeled as positive, negative, or neutral. Text preprocessing includes:\n",
    "- Converting to lowercase\n",
    "- Removing punctuation\n",
    "- Lemmatization\n",
    "- Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f38f3ba7-d0a0-4d42-8c8b-42244c3313ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing function\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "\n",
    "# Load and preprocess dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        \"This product is amazing and I love it!\", \"Terrible service, very disappointed.\",\n",
    "        \"It's okay, nothing special.\", \"Fantastic experience, highly recommend!\",\n",
    "        \"Worst purchase ever, waste of money.\", \"Really happy with my purchase!\",\n",
    "        \"Not bad, could be better.\", \"Horrible quality, broke after one use.\",\n",
    "        \"I’m neutral about this product.\", \"Superb quality, worth every penny!\",\n",
    "        \"Service was okay, not great.\", \"Very bad experience, never again.\",\n",
    "        \"I love this product so much!\", \"Quality is decent, nothing extraordinary.\",\n",
    "        \"This item is fantastic, I’m thrilled!\", \"Really disappointed, poor service.\",\n",
    "        \"It works fine, nothing to complain about.\", \"Awful product, total waste.\",\n",
    "        \"Great product, very satisfied!\", \"Neutral opinion, it’s just okay.\",\n",
    "        \"Bad quality, not worth it.\", \"Amazing service, will buy again!\",\n",
    "        \"Mediocre experience, not impressed.\", \"Terrible product, broke quickly.\",\n",
    "        \"Love the quality, highly recommend!\", \"Poor service, very upset.\",\n",
    "        \"It’s fine, no strong opinion.\", \"Awesome product, super happy!\",\n",
    "        \"Really bad, not recommended.\", \"Just okay, nothing exciting.\",\n",
    "        \"Incredible product, highly satisfied!\", \"Very poor quality, total failure.\",\n",
    "        \"Neutral feedback, it’s average.\", \"Best purchase ever, love it!\",\n",
    "        \"Terrible experience, not worth it.\", \"Nothing special, just fine.\",\n",
    "        \"Wonderful product, very pleased!\", \"Disappointing quality, not great.\",\n",
    "        \"It’s alright, nothing to write home about.\", \"Top-notch service, love it!\",\n",
    "        \"Complete waste, very unhappy.\", \"Pretty average, no complaints.\",\n",
    "        \"Excellent quality, highly recommend!\", \"Bad service, very frustrating.\",\n",
    "        \"Neutral, it’s just okay.\"\n",
    "    ],\n",
    "    'sentiment': ['positive', 'negative', 'neutral', 'positive', 'negative', \n",
    "                  'positive', 'neutral', 'negative', 'neutral', 'positive',\n",
    "                  'neutral', 'negative', 'positive', 'neutral', 'positive', \n",
    "                  'negative', 'neutral', 'negative', 'positive', 'neutral',\n",
    "                  'negative', 'positive', 'neutral', 'negative', 'positive', \n",
    "                  'negative', 'neutral', 'positive', 'negative', 'neutral',\n",
    "                  'positive', 'negative', 'neutral', 'positive', 'negative', \n",
    "                  'neutral', 'positive', 'negative', 'neutral', 'positive', \n",
    "                  'negative', 'neutral', 'positive', 'negative', 'neutral']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Prepare features and target\n",
    "X = df['text']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be1f65-a161-479e-adbf-8eaca682f4c7",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "The model uses a pipeline combining TF-IDF vectorization and Logistic Regression with balanced class weights. The dataset is split into 60% training and 40% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7327b8b-c65a-477f-8dc0-94d55769e7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidfvectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_df=0.9, ngram_range=(1, 3),\n",
       "                                 stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logisticregression&#x27;,\n",
       "                 LogisticRegression(C=0.5, class_weight=&#x27;balanced&#x27;,\n",
       "                                    max_iter=200))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidfvectorizer&#x27;,\n",
       "                 TfidfVectorizer(max_df=0.9, ngram_range=(1, 3),\n",
       "                                 stop_words=&#x27;english&#x27;)),\n",
       "                (&#x27;logisticregression&#x27;,\n",
       "                 LogisticRegression(C=0.5, class_weight=&#x27;balanced&#x27;,\n",
       "                                    max_iter=200))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.9, ngram_range=(1, 3), stop_words=&#x27;english&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.5, class_weight=&#x27;balanced&#x27;, max_iter=200)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidfvectorizer',\n",
       "                 TfidfVectorizer(max_df=0.9, ngram_range=(1, 3),\n",
       "                                 stop_words='english')),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=0.5, class_weight='balanced',\n",
       "                                    max_iter=200))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Build and train model\n",
    "model = make_pipeline(\n",
    "    TfidfVectorizer(stop_words='english', ngram_range=(1, 3), min_df=1, max_df=0.9),\n",
    "    LogisticRegression(max_iter=200, class_weight='balanced', C=0.5)\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5495d27e-cae1-4315-a018-9a7f9d1d854e",
   "metadata": {},
   "source": [
    "## Evaluation and Predictions\n",
    "\n",
    "The model is evaluated using accuracy and cross-validation. Predictions are made on the test set and new texts. The current accuracy is 0.666, indicating room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab10a0f4-7fb7-4219-a297-014c98b7e2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: ['superb quality worth every penny' 'work fine nothing complain'\n",
      " 'disappointing quality great' 'poor quality total failure'\n",
      " 'awesome product super happy' 'product amazing love'\n",
      " 'incredible product highly satisfied' 'okay nothing exciting'\n",
      " 'really happy purchase' 'bad experience never' 'best purchase ever love'\n",
      " 'terrible service disappointed' 'complete waste unhappy'\n",
      " 'amazing service buy' 'okay nothing special' 'terrible experience worth'\n",
      " 'terrible product broke quickly' 'wonderful product pleased'\n",
      " 'service okay great' 'mediocre experience impressed'\n",
      " 'great product satisfied' 'neutral okay' 'bad quality worth'\n",
      " 'horrible quality broke one use' 'item fantastic im thrilled'\n",
      " 'really bad recommended' 'alright nothing write home']\n",
      "y_train: ['positive' 'neutral' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'neutral' 'positive' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'neutral' 'negative' 'negative' 'positive'\n",
      " 'neutral' 'neutral' 'positive' 'neutral' 'negative' 'negative' 'positive'\n",
      " 'negative' 'neutral']\n",
      "X_test: ['topnotch service love' 'poor service upset' 'fine strong opinion'\n",
      " 'bad service frustrating' 'nothing special fine'\n",
      " 'pretty average complaint' 'worst purchase ever waste money'\n",
      " 'love product much' 'im neutral product'\n",
      " 'fantastic experience highly recommend' 'bad could better'\n",
      " 'love quality highly recommend' 'neutral feedback average'\n",
      " 'neutral opinion okay' 'awful product total waste'\n",
      " 'quality decent nothing extraordinary'\n",
      " 'excellent quality highly recommend' 'really disappointed poor service']\n",
      "y_test: ['positive' 'negative' 'neutral' 'negative' 'neutral' 'neutral' 'negative'\n",
      " 'positive' 'neutral' 'positive' 'neutral' 'positive' 'neutral' 'neutral'\n",
      " 'negative' 'neutral' 'positive' 'negative']\n",
      "Predictions: ['positive' 'negative' 'neutral' 'negative' 'neutral' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'neutral' 'neutral' 'negative' 'negative' 'negative' 'negative']\n",
      "Text: topnotch service love, True: positive, Predicted: positive\n",
      "Text: poor service upset, True: negative, Predicted: negative\n",
      "Text: fine strong opinion, True: neutral, Predicted: neutral\n",
      "Text: bad service frustrating, True: negative, Predicted: negative\n",
      "Text: nothing special fine, True: neutral, Predicted: neutral\n",
      "Text: pretty average complaint, True: neutral, Predicted: positive\n",
      "Text: worst purchase ever waste money, True: negative, Predicted: positive\n",
      "Text: love product much, True: positive, Predicted: positive\n",
      "Text: im neutral product, True: neutral, Predicted: positive\n",
      "Text: fantastic experience highly recommend, True: positive, Predicted: positive\n",
      "Text: bad could better, True: neutral, Predicted: negative\n",
      "Text: love quality highly recommend, True: positive, Predicted: positive\n",
      "Text: neutral feedback average, True: neutral, Predicted: neutral\n",
      "Text: neutral opinion okay, True: neutral, Predicted: neutral\n",
      "Text: awful product total waste, True: negative, Predicted: negative\n",
      "Text: quality decent nothing extraordinary, True: neutral, Predicted: negative\n",
      "Text: excellent quality highly recommend, True: positive, Predicted: negative\n",
      "Text: really disappointed poor service, True: negative, Predicted: negative\n",
      "Accuracy: 0.6666666666666666\n",
      "Cross-validation accuracy: 0.8222222222222223\n",
      "New text predictions: ['positive' 'positive']\n",
      "Top features: ['alright' 'alright write' 'alright write home' 'amazing' 'amazing love'\n",
      " 'amazing service' 'amazing service buy' 'awesome' 'awesome product'\n",
      " 'awesome product super' 'bad' 'bad experience' 'bad quality'\n",
      " 'bad quality worth' 'bad recommended' 'best' 'best purchase'\n",
      " 'best purchase love' 'broke' 'broke quickly']\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print detailed results\n",
    "print(\"X_train:\", X_train.values)\n",
    "print(\"y_train:\", y_train.values)\n",
    "print(\"X_test:\", X_test.values)\n",
    "print(\"y_test:\", y_test.values)\n",
    "print(\"Predictions:\", predictions)\n",
    "for text, true_label, pred_label in zip(X_test, y_test, predictions):\n",
    "    print(f\"Text: {text}, True: {true_label}, Predicted: {pred_label}\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "\n",
    "# Cross-validation\n",
    "scores = cross_val_score(model, X, y, cv=3, scoring='accuracy')\n",
    "print(\"Cross-validation accuracy:\", scores.mean())\n",
    "\n",
    "# Predict new texts\n",
    "new_texts = [\"I really enjoyed using this!\", \"This is awful, never again.\"]\n",
    "new_texts = [preprocess_text(text) for text in new_texts]\n",
    "new_predictions = model.predict(new_texts)\n",
    "print(\"New text predictions:\", new_predictions)\n",
    "\n",
    "# Debug TF-IDF features\n",
    "vectorizer = model.named_steps['tfidfvectorizer']\n",
    "print(\"Top features:\", vectorizer.get_feature_names_out()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de49c32d-077e-42a8-829d-f0440b98772d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook implements a sentiment analysis model with an accuracy of 0.666. The low accuracy is due to the small dataset size (45 samples). Future improvements include:\n",
    "- Using a larger dataset like Twitter Sentiment.\n",
    "- Applying advanced models like BERT.\n",
    "- Enhancing text preprocessing (e.g., removing URLs or emojis).\n",
    "\n",
    "Next steps involve testing the model with real-world data and deploying it as a web app using Streamlit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
